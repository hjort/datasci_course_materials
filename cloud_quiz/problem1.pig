/*
Problem 1: Getting started with Pig on chunk-000

- Use the file uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-000 
- Run on an AWS cluster with 10 small nodes

1.1 How many MapReduce jobs are generated by example.pig? 1 job

1.2 How many reduce tasks are within the first MapReduce job? How many reduce tasks are within later MapReduce jobs? 33 map tasks, 50 reduce tasks

1.3 How long does each job take? How long does the entire script take? 5mins, 55sec

1.4 What is the schema of the tuples after each of the following commands in example.pig?

After the command ntriples = ...
After the command objects = ...
After the command count_by_object = ...

What you need to turn in:
How many records are there in count_by_object?
*/

REGISTER s3n://uw-cse-344-oregon.aws.amazon.com/myudfs.jar

-- load the test file into Pig
raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/cse344-test-file' USING TextLoader AS (line:chararray);
-- later you will load to other files, example:
--raw = LOAD 's3n://uw-cse-344-oregon.aws.amazon.com/btc-2010-chunk-000' USING TextLoader AS (line:chararray); 
DESCRIBE raw; -- raw: {line: chararray}

-- parse each line into ntriples
ntriples = FOREACH raw GENERATE FLATTEN(myudfs.RDFSplit3(line)) AS (subject:chararray, predicate:chararray, object:chararray);
DESCRIBE ntriples; -- ntriples: {subject: chararray,predicate: chararray,object: chararray}

-- group the n-triples by object column
objects = GROUP ntriples BY (object) PARALLEL 50;
DESCRIBE objects;
-- objects: {group: chararray,ntriples: {(subject: chararray,predicate: chararray,object: chararray)}}

-- flatten the objects out (because group by produces a tuple of each object
-- in the first column, and we want each object ot be a string, not a tuple),
-- and count the number of tuples associated with each object
count_by_object = FOREACH objects GENERATE FLATTEN($0), COUNT($1) AS count PARALLEL 50;
DESCRIBE count_by_object; -- count_by_object: {group: chararray,count: long}

-- based on http://stackoverflow.com/questions/9900761/pig-how-to-count-a-number-of-rows-in-alias
cbo_group = GROUP count_by_object ALL;
cbo_count = FOREACH cbo_group GENERATE COUNT (count_by_object);

DESCRIBE cbo_group; -- cbo_group: {group: chararray,count_by_object: {(group: chararray,count: long)}}
DESCRIBE cbo_count; -- cbo_count: {long}

--order the resulting tuples by their count in descending order
--count_by_object_ordered = ORDER count_by_object BY (count) PARALLEL 50;

-- store the results in the folder /user/hadoop/example-results
--store count_by_object_ordered into '/user/hadoop/example-results' using PigStorage();
-- alternatively, you can store the results in S3, see instructions:
--store count_by_object_ordered into 's3n://superman/example-results';
STORE cbo_count INTO 's3n://hjort-mapreduce/problem1';

